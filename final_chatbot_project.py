# -*- coding: utf-8 -*-
"""Final chatbot_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1peeqZgY0cnLfUqIN7fDE8a6v2La-dbwO
"""

import numpy as np
import pandas as pd
# For visualizations
import matplotlib.pyplot as plt
# For regular expressions
import re
# For handling string
import string
# For performing mathematical operations
import math

from google.colab import files
 
uploaded = files.upload()

import pandas as pd
import io
 
df = pd.read_csv(io.BytesIO(uploaded['BankFAQs.csv']))
print(df)

df.head()

df.info()

df.isnull().sum()

df.dropna(inplace=True)
df.isnull().sum()

df['Question'].unique()

df['Question']=df['Question'].apply(lambda x: x.split(',,,')[0])
df['Question']

keys=df['Class']

df['Question'].str.len().hist()

acc=df[df['Class']=='accounts']
acc

len(acc)

card=df[df['Class']=='cards']
card

fund=df[df['Class']=='fundstransfer']
fund

len(fund)

insu=df[df['Class']=='insurance']
insu

invest=df[df['Class']=='investments']
invest

loan=df[df['Class']=='loans']
loan

secu=df[df['Class']=='security']
secu

# Creating dataset
classes = ['acc', 'card', 'fund',
        'secu', 'loan', 'invest','insu']
 
data = [len(acc), len(card), len(fund), len(secu), len(loan), len(invest), len(insu)]
 
# Creating plot
fig = plt.figure(figsize =(10, 7))
plt.pie(data, labels = classes)
 
# show plot
plt.show()

classes = ['acc', 'card', 'fund',
        'secu', 'loan', 'invest','insu']
 
data = [len(acc), len(card), len(fund), len(secu), len(loan), len(invest), len(insu)]
explode = (0.1, 0.0, 0.2, 0.3, 0.0, 0.0,0.0)
 
# Creating color parameters
colors = ( "orange", "cyan", "blue",
          "grey", "yellow", "beige","pink")
 
# Wedge properties
wp = { 'linewidth' : 1, 'edgecolor' : "green" }
 
# Creating autocpt arguments
def func(pct, allvalues):
    absolute = int(pct / 100.*np.sum(allvalues))
    return "{:.1f}%\n({:d} g)".format(pct, absolute)
 
# Creating plot
fig, ax = plt.subplots(figsize =(10, 7))
wedges, texts, autotexts = ax.pie(data,
                                  autopct = lambda pct: func(pct, data),
                                  explode = explode,
                                  labels = classes,
                                  shadow = True,
                                  colors = colors,
                                  startangle = 90,
                                  wedgeprops = wp,
                                  textprops = dict(color ="magenta"))
 
# Adding legend
ax.legend(wedges, classes,
          title ="Categories of Questions",
          loc ="center left",
          bbox_to_anchor =(1, 0, 0.5, 1))
 
plt.setp(autotexts, size = 8, weight ="bold")
ax.set_title("Categorical analysis of questions")
 
# show plot
plt.show()

import nltk
from nltk.corpus import stopwords
import re
nltk.download('stopwords')

stop=set(stopwords.words('english'))

corpus=[]
new= df['Question'].str.split()
new=new.values.tolist()
corpus=[word for i in new for word in i]
print(corpus)
from collections import defaultdict
dic=defaultdict(int)
for word in corpus:
    if word in stop:
        dic[word]+=1
print(dic)

top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] 
x,y=zip(*top)
plt.bar(x,y)

import collections 
from collections import Counter
import seaborn as sns
counter=Counter(corpus)
most=counter.most_common()

x, y= [], []
for word,count in most[:40]:
    if (word not in stop):
        x.append(word)
        y.append(count)
        
sns.barplot(x=y,y=x)



"""## **DATA PREPROCESSING**"""

def remove_punctuation(text):
    no_punct=[words for words in text if words not in string.punctuation ]
    words_wo_punct=''.join(no_punct)
    return words_wo_punct

df['text_wo_punct']=df['Question'].apply(lambda x: remove_punctuation(x))
df['text_wo_punct_ans']=df['Answer'].apply(lambda x: remove_punctuation(x))

df.head()

###TOKENISATION

def tokenize(text):
    split=re.split("\W+",text) 
    return split
df['text_wo_punct_split']=df['text_wo_punct'].apply(lambda x: tokenize(x))
df['text_wo_punct_ans_split']=df['text_wo_punct_ans'].apply(lambda x: tokenize(x))
df.head()

#########STOP WORDS REMOVING
stopword = nltk.corpus.stopwords.words('english')
def remove_stopwords(text):
    text=[word for word in text if word not in stopword]
    return text

df['text_wo_punct_split_wo_stopwords']=df['text_wo_punct_split'].apply(lambda x: remove_stopwords(x))
df['text_wo_punct_split_wo_ans_stopwords']=df['text_wo_punct_ans_split'].apply(lambda x: remove_stopwords(x))
df.head()

import nltk
nltk.download('omw-1.4')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
  
lemmatizer=nltk.stem.WordNetLemmatizer()

def lemmatize_text(word_list):
    
    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
    return lemmatized_output


df['text_wo_punct_split_wo_stopwords_lim']=df['text_wo_punct_split_wo_stopwords'].apply(lambda x: lemmatize_text(x))
df['text_wo_punct_split_wo_ans_stopwords_lim']=df['text_wo_punct_split_wo_ans_stopwords'].apply(lambda x: lemmatize_text(x))
df.head()

from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
stemmer = PorterStemmer()
 
# stem words in the list of tokenized words
def stem_words(text):
    #print(text)
    stems = stemmer.stem(text) 
    return stems
df['text_wo_punct_split_wo_stopwords_stem']=df['text_wo_punct_split_wo_stopwords_lim'].apply(lambda x: stem_words(x))
df['text_wo_punct_split_wo_ans_stopwords_stem']=df['text_wo_punct_split_wo_ans_stopwords_lim'].apply(lambda x: stem_words(x))
df.head()

def text_clean(text):
    text = re.sub(r'^https?:\/\/.*[\r\n]*', '', text, flags=re.MULTILINE)
    text = re.sub('<.*?>+', '', text)
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags = re.UNICODE)
    text = regrex_pattern.sub(r'',text)
    text = ''.join([i for i in text if not i.isdigit()])
    return text
df['clean_text']=df['text_wo_punct_split_wo_stopwords_lim'].apply(lambda x: text_clean(x))
df['clean_text_ans']=df['text_wo_punct_split_wo_ans_stopwords_lim'].apply(lambda x: text_clean(x))
df.head()

import pandas as pd
from nltk import word_tokenize
from nltk.stem.lancaster import LancasterStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

X=df['clean_text']
tfv = TfidfVectorizer(stop_words='english')
tfv.fit(X)
X = tfv.transform(X)

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity

lemmatizer = WordNetLemmatizer()
le = LabelEncoder()
le.fit(df['Class'])
y = le.transform(df['Class'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=42)

model = SVC(kernel='linear')
model.fit(X_train, y_train)
print(model.score(X_train, y_train))

print(model.score(X_test, y_test))

l = X[100]
print(l)
print(df['Question'][100])
t_usr = l # tfv.transform([clean(l)])
class_ = le.inverse_transform(model.predict(t_usr))[0]
print(f"Predicted Class:- {class_}")

questions = df[df['Class']==class_]
cos_sims = []
for ques in questions['Question']:
 sims = cosine_similarity(tfv.transform([ques]), t_usr)
 cos_sims.append(sims)
ind = cos_sims.index(max(cos_sims))
print(f"Similar question from the dataset :- {questions['Question'][questions.index[ind]]}")

questions['Answer'][questions.index[ind]]

from sklearn import model_selection, naive_bayes, svm
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
Naive = naive_bayes.MultinomialNB()
Naive.fit(X_train,y_train)
# predict the labels on validation dataset
predictions_NB = Naive.predict(X_test)
# Use accuracy_score function to get the accuracy
print("Naive Bayes Accuracy Score -> ",accuracy_score(predictions_NB, y_test)*100)

l = X[100]
#print(l)
print(df['Question'][100])
t_usr = l # tfv.transform([clean(l)])
class_ = le.inverse_transform(Naive.predict(t_usr))[0]
print(f"Predicted Class:- {class_}")
questions = df[df['Class']==class_]
cos_sims = []
for ques in questions['Question']:
 sims = cosine_similarity(tfv.transform([ques]), t_usr)
 cos_sims.append(sims)
ind = cos_sims.index(max(cos_sims))
print(f"Similar question from the dataset :- {questions['Question'][questions.index[ind]]}")
questions['Answer'][questions.index[ind]]

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier()
classifier = classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
print(classifier.score(X_train, y_train))
print(accuracy_score(y_pred, y_test))
print(classifier.score(X_test,y_test))

l = X[100]
#print(l)
print(df['Question'][100])
t_usr = l # tfv.transform([clean(l)])
class_ = le.inverse_transform(classifier.predict(t_usr))[0]
print(f"Predicted Class:- {class_}")
questions = df[df['Class']==class_]
cos_sims = []
for ques in questions['Question']:
 sims = cosine_similarity(tfv.transform([ques]), t_usr)
 cos_sims.append(sims)
ind = cos_sims.index(max(cos_sims))
print(f"Similar question from the dataset :- {questions['Question'][questions.index[ind]]}")
questions['Answer'][questions.index[ind]]

import pickle
import pickle
with open('model.pickle', 'wb') as fid:
    pickle.dump(model, fid) 
#save- pickle.dump(model, open('model.pkl', 'wb'))

with open('./model.pickle', 'rb') as f:
        obj = pickle.load(f)
obj.predict(X_test)

classifier.save("chatbot_model.h5", hist)
print("model created")

